{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPqdcvFlGb9wBcUqdocIo9L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rocketwolf98/ML_experiments/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhPdBn6tNF3I"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = tf.keras.utils.get_file('shakespeare.txt','https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "text = open(filepath, 'rb').read().decode(encoding='utf-8').lower()"
      ],
      "metadata": {
        "id": "t1HSB9quPOLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25c81b9b-12ae-42fd-dae2-d97741788070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = text[300000:800000]"
      ],
      "metadata": {
        "id": "J83oZobuQp28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "characters= sorted(set(text))"
      ],
      "metadata": {
        "id": "SOP0XylASIZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "77zma8RlY4PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_index = dict((c, i) for i, c in enumerate(characters))\n",
        "index_to_char = dict((i, c) for i, c in enumerate(characters))"
      ],
      "metadata": {
        "id": "g55VCwuZTV-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LENGTH = 40\n",
        "STEP_SIZE = 3"
      ],
      "metadata": {
        "id": "wL-VguMtUKZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "next_characters = []"
      ],
      "metadata": {
        "id": "OTuGiSNyTl-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(text) - SEQ_LENGTH, STEP_SIZE):\n",
        "  sentences.append(text[i: i+SEQ_LENGTH])\n",
        "  next_characters.append(text[i+SEQ_LENGTH])"
      ],
      "metadata": {
        "id": "uRVwot5gUlCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.zeros((len(sentences), SEQ_LENGTH, len(characters)), dtype=bool)\n",
        "y = np.zeros((len(sentences), len(characters)), dtype= bool)\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "  for t, character in enumerate(sentence):\n",
        "    x[i, t, char_to_index[character]] = 1\n",
        "  y[i, char_to_index[next_characters[i]]] = 1"
      ],
      "metadata": {
        "id": "kP2pNIPEVG_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building Neural Network"
      ],
      "metadata": {
        "id": "6uOEyb4HXBFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()"
      ],
      "metadata": {
        "id": "N8ZMOK4_XDeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(LSTM(128, input_shape = (SEQ_LENGTH, len(characters))))\n",
        "model.add(Dense(len(characters)))\n",
        "model.add(Activation(\"softmax\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZgR6E-mXJei",
        "outputId": "0e40927a-6163-4348-a24d-5d5ef3c6f470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(learning_rate=0.01))\n",
        "model.fit(x, y, batch_size=256, epochs=4)\n",
        "model.save('textgenerator.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9Klpie8XtJd",
        "outputId": "97e4990c-c4b7-4553-89c0-3f5139f5744b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 2.4769\n",
            "Epoch 2/4\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 1.7606\n",
            "Epoch 3/4\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 1.6061\n",
            "Epoch 4/4\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 1.5192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Generator"
      ],
      "metadata": {
        "id": "f2JYqmLgfzI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype(\"float64\")\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "metadata": {
        "id": "JvrN8v7oZYOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(length, temperature):\n",
        "    start_index = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "    generated = ''\n",
        "    sentence = text[start_index: start_index + SEQ_LENGTH]\n",
        "    generated += sentence\n",
        "    for i in range(length):\n",
        "        x_predictions = np.zeros((1, SEQ_LENGTH, len(characters)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_predictions[0, t, char_to_index[char]] = 1\n",
        "\n",
        "        predictions = model.predict(x_predictions, verbose=0)[0]\n",
        "        next_index = sample(predictions,\n",
        "                                 temperature)\n",
        "        next_character = index_to_char[next_index]\n",
        "\n",
        "        generated += next_character\n",
        "        sentence = sentence[1:] + next_character\n",
        "    return generated"
      ],
      "metadata": {
        "id": "wwRHJllXfn2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"for temperature 0.2\")\n",
        "print(generate_text(300, 0.2))\n",
        "print(\"for temperature 0.4\")\n",
        "print(generate_text(300, 0.4))\n",
        "print(\"for temperature 0.6\")\n",
        "print(generate_text(300, 0.6))\n",
        "print(\"for temperature 0.8\")\n",
        "print(generate_text(300, 0.8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZmXH0g1hWpQ",
        "outputId": "a3ef6021-3d4e-411b-b119-ffd757acdbd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for temperature 0.2\n",
            "pardon of the prince, and call thee back.\n",
            "\n",
            "king richard iii:\n",
            "the can that honour of the casile shall not thine the sease\n",
            "that the comes the brother of the love,\n",
            "that he have not thy love in the land his canst the canst the sease.\n",
            "\n",
            "king richard iii:\n",
            "why, thou death that the fair death my father\n",
            "that a grave that the sen the comes the sease\n",
            "for temperature 0.4\n",
            "f york:\n",
            "nay, do not say, 'stand up;'\n",
            "say not thy brother, that shall he should have the sease,\n",
            "the warm, and can princh romeo and not shower.\n",
            "\n",
            "king richard iii:\n",
            "good more, that this grace, which son the consent\n",
            "thou was thine the hand thy bears this land\n",
            "thy death death, not that where i may thine so him.\n",
            "\n",
            "king richard iii:\n",
            "why, so i shal\n",
            "for temperature 0.6\n",
            " edward iv:\n",
            "welcome, sir john! but why confend use.\n",
            "\n",
            "thomas of york:\n",
            "but thou love, whose shall me worn for a grief,\n",
            "this that i child stick proke hence to the want.\n",
            "\n",
            "king richard iii:\n",
            "hath than the warsick i praise thy son,\n",
            "that i have sead thy nore that thy death,\n",
            "and make him of this sun to part: and the seconse\n",
            "manour bolingbroke of t\n",
            "for temperature 0.8\n",
            "d thanks, and let's away to london\n",
            "and stirs his swring in thine warwick-soulls;\n",
            "'tis pain to he hand the early you'd queen\n",
            "be of thy montague's breathe thy monthrow\n",
            "no less pard with whildrenter to in.\n",
            "\n",
            "henry bolingbroke:\n",
            "\n",
            "farwick:\n",
            "thou hast be high day devise with the are hand\n",
            "is neigher thou not bolingbroke!\n",
            "\n",
            "mencamie:\n",
            "i woll vicland b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5TL0s8Z2hl0i"
      }
    }
  ]
}